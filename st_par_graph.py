import os
import shutil
from glob import glob
import streamlit as st
from dotenv import load_dotenv

# LangChain ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ­ãƒ¼ãƒ€ãƒ¼ã¨ã‚¹ãƒ—ãƒªãƒƒã‚¿ãƒ¼
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain.text_splitter import CharacterTextSplitter

# åŸ‹ã‚è¾¼ã¿ã¨ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_ollama import OllamaLLM
from langchain_ollama import ChatOllama
from langchain_huggingface import HuggingFaceEmbeddings
# from langchain_community.vectorstores import Chroma
from langchain_chroma import Chroma
# LLM ãƒ¢ãƒ‡ãƒ«
from langchain_community.chat_models import AzureChatOpenAI
from langchain.prompts import PromptTemplate

import os
import torch

torch.classes.__path__ = [os.path.join(torch.__path__[0], torch.classes.__file__)]

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

# ======== è¨­å®šå€¤ã‚’ã¾ã¨ã‚ã¦ç®¡ç† ========
def get_config():
    return {
        # LLMã‚µãƒ¼ãƒ“ã‚¹: 'azure', 'openai', 'ollama'
        "LLM_SERVICE": "ollama",
        "AZURE_DEPLOYMENT_NAME": "gpt-4o",
        "AZURE_API_VERSION": "2024-04-01",
        "OPENAI_MODEL": "gpt-4o",
        "OLLAMA_MODEL": "gemma3:4b-it-qat",
        "OLLAMA_BASE_URL": "http://localhost:11434",

        # åŸ‹ã‚è¾¼ã¿ã‚µãƒ¼ãƒ“ã‚¹: 'azure', 'openai', 'hf'
        "EMBEDDING_SERVICE": "hf",
        "AZURE_EMBEDDING_DEPLOYMENT": "text-embedding-ada-002",
        "AZURE_EMBEDDING_API_VERSION": "2024-04-01",
        "OPENAI_EMBEDDING_MODEL": "text-embedding-3-small",
        "HF_EMBEDDING_MODEL": "intfloat/multilingual-e5-small",

        # ãã®ä»–
        "PDF_FOLDER": "pdfs",
        "CHROMA_PATH": "chroma_db",
    }

# ======== ãƒ¢ãƒ‡ãƒ«ã¨åŸ‹ã‚è¾¼ã¿è¨­å®š ========
@st.cache_resource
def load_embeddings(config):
    service = config.get("EMBEDDING_SERVICE", "hf")
    if service == "azure":
        try:
            return OpenAIEmbeddings(
                deployment=config["AZURE_EMBEDDING_DEPLOYMENT"],
                openai_api_version=config["AZURE_EMBEDDING_API_VERSION"]
            )
        except Exception:
            st.error("Azure OpenAIåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            st.stop()
    elif service == "openai":
        try:
            return OpenAIEmbeddings(
                model=config["OPENAI_EMBEDDING_MODEL"]
            )
        except Exception:
            st.error("OpenAIåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            st.stop()
    elif service == "hf":
        try:
            return HuggingFaceEmbeddings(model_name=config["HF_EMBEDDING_MODEL"])
        except Exception:
            st.error("HuggingFaceåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            st.stop()
    else:
        st.error("åŸ‹ã‚è¾¼ã¿ã‚µãƒ¼ãƒ“ã‚¹ã®è¨­å®šãŒä¸æ­£ã§ã™ã€‚")
        st.stop()

def load_llm(config):
    service = config.get("LLM_SERVICE", "ollama")
    if service == "azure":
        return AzureChatOpenAI(
            deployment_name=config["AZURE_DEPLOYMENT_NAME"],
            openai_api_version=config["AZURE_API_VERSION"]
        )
    elif service == "openai":
        return ChatOllama(model=config["OPENAI_MODEL"])
    elif service == "ollama":
        return ChatOllama(
            model=config["OLLAMA_MODEL"],
            base_url=config["OLLAMA_BASE_URL"]
        )
    else:
        st.error("LLMã‚µãƒ¼ãƒ“ã‚¹ã®è¨­å®šãŒä¸æ­£ã§ã™ã€‚")
        st.stop()

# ======== PDFãƒ•ã‚©ãƒ«ãƒ€èª­ã¿è¾¼ã¿ & ChromaDB æ§‹ç¯‰ ========
def build_chroma(embeddings, config):
    loader = PyPDFDirectoryLoader(
        path=config["PDF_FOLDER"],
    )
    splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    docs = loader.load_and_split(text_splitter=splitter)
    st.write(f"PDF chunk æ•°: {len(docs)} ä»¶")
    if len(docs) == 0:
        st.error("PDFãŒ1ä»¶ã‚‚èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚pdfsãƒ•ã‚©ãƒ«ãƒ€ã«PDFãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return None
    vectorstore = Chroma.from_documents(
        documents=docs,
        embedding=embeddings,
        persist_directory=config["CHROMA_PATH"]
    )
    return vectorstore

# ======== Plan ã‚¹ãƒ†ãƒƒãƒ— ========
from pydantic import BaseModel, Field

class PlanSteps(BaseModel):
    steps: list[str] = Field(description="è³ªå•ã«ç­”ãˆã‚‹ãŸã‚ã®æ®µéšçš„ãªæ¤œç´¢ã‚¹ãƒ†ãƒƒãƒ—")

def get_planner_chain(llm):
    plan_prompt = PromptTemplate(
        input_variables=["question"],
        template="""
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã‚‹ãŸã‚ã«ã€æ®µéšçš„ã«æ¤œç´¢ã‚¹ãƒ†ãƒƒãƒ—ã‚’è¨ˆç”»ã—ã¦ãã ã•ã„ã€‚
è³ªå•: {question}
"""
    )
    return plan_prompt | llm.with_structured_output(PlanSteps)

# ======== Act & Answer ã‚¹ãƒ†ãƒƒãƒ— ========
def retrieve_and_answer(vectorstore, steps, llm, max_retry=2):
    class StepReview(BaseModel):
        result: str = Field(description="OKãªã‚‰'OK'ã€ä¸ååˆ†ãƒ»èª¤ã‚ŠãŒã‚ã‚Œã°'NG'")
        reason: str = Field(description="NGã®å ´åˆã®ç†ç”±ã€‚OKãªã‚‰ç©ºæ–‡å­—")
        new_query: str = Field(description="NGã®å ´åˆã®å†æ¤œç´¢ã‚¯ã‚¨ãƒªæ¡ˆã€‚OKãªã‚‰ç©ºæ–‡å­—")

    results = []
    for i, step in enumerate(steps, start=1):
        current_step = step
        for retry in range(max_retry):
            st.markdown(f"### Step {i} {step}- Try: {retry+1}")
            docs = vectorstore.similarity_search(current_step, k=3)
            context = "\n".join([d.page_content for d in docs])
            step_prompt = PromptTemplate(
                input_variables=["step", "context"],
                template="""
ä»¥ä¸‹ã®æƒ…å ±ã«åŸºã¥ã„ã¦ä¸­é–“å›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
ã‚¹ãƒ†ãƒƒãƒ—: {step}
å‚ç…§:
{context}

å›ç­”:
"""
            )
            step_chain = step_prompt | llm
            answer = step_chain.invoke({"step": current_step, "context": context})
            st.info(f"**ä¸­é–“å›ç­”:** {answer.content}")

            # --- RAGã«ã‚ˆã‚‹ä¸­é–“å›ç­”ã®æ­£ç¢ºæ€§ãƒã‚§ãƒƒã‚¯ ---
            review_prompt = PromptTemplate(
                input_variables=["step", "answer", "context"],
                template="""
ä»¥ä¸‹ã¯ã€ã‚ã‚‹æ¤œç´¢ã‚¹ãƒ†ãƒƒãƒ—ã«å¯¾ã™ã‚‹ä¸­é–“å›ç­”ã§ã™ã€‚

ã‚¹ãƒ†ãƒƒãƒ—: {step}
å›ç­”: {answer}
å‚ç…§æƒ…å ±:
{context}

ã“ã®å›ç­”ã¯ååˆ†ã«æ­£ç¢ºãƒ»å¦¥å½“ã§ã™ã‹ï¼Ÿ
- ååˆ†ãªã‚‰ result: OK, reason: , new_query: 
- ä¸ååˆ†ãƒ»èª¤ã‚ŠãŒã‚ã‚Œã° result: NG, reason: (ç†ç”±), new_query: (å†æ¤œç´¢ã‚¯ã‚¨ãƒªæ¡ˆ)
å‡ºåŠ›ã¯å¿…ãšJSONå½¢å¼ã§:
{{
  \"result\": ...,
  \"reason\": ...,
  \"new_query\": ...
}}
"""
            )
            review_chain = review_prompt | llm.with_structured_output(StepReview)
            review_result = review_chain.invoke({"step": current_step, "answer": answer, "context": context})

            st.markdown(f"**ãƒ¬ãƒ“ãƒ¥ãƒ¼åˆ¤å®š:** {review_result.result}")
            if review_result.result.strip().upper() == "OK":
                st.success("OK: ååˆ†ãªå›ç­”ã§ã™ã€‚")
                results.append((current_step, answer))
                break
            else:
                st.warning(f"NG: {review_result.reason}")
                if review_result.new_query and review_result.new_query != current_step:
                    st.info(f"å†æ¤œç´¢ã‚¯ã‚¨ãƒªæ¡ˆ: {review_result.new_query}")
                    current_step = review_result.new_query
                if retry == max_retry - 1:
                    st.error("ãƒªãƒˆãƒ©ã‚¤ä¸Šé™ã«é”ã—ãŸãŸã‚ã€ã“ã®å›ç­”ã‚’æ¡ç”¨ã—ã¾ã™ã€‚")
                    results.append((current_step, answer))
    return results

# ======== Review ã‚¹ãƒ†ãƒƒãƒ— ========
def review_steps(step_results, llm):
    review_prompt = PromptTemplate(
        input_variables=["steps"],
        template="""
ä»¥ä¸‹ã¯å„ã‚¹ãƒ†ãƒƒãƒ—ã®ä¸­é–“å›ç­”ã§ã™ã€‚æ•´åˆæ€§ã‚„ä¸æ˜ç‚¹ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã¦ãã ã•ã„ã€‚

{steps}

ãƒ¬ãƒ“ãƒ¥ãƒ¼çµæœ:
"""
    )
    steps_text = "\n".join([f"{i}. {s} â†’ {a}" for i, (s, a) in enumerate(step_results, start=1)])
    review_chain = review_prompt | llm
    return review_chain.invoke({"steps": steps_text})

# ======== Aggregate ã‚¹ãƒ†ãƒƒãƒ— ========
def aggregate_answer(question, step_results, review_notes, llm):
    agg_prompt = PromptTemplate(
        input_variables=["question", "steps", "review"],
        template="""
è³ªå•: {question}

å„ã‚¹ãƒ†ãƒƒãƒ—å›ç­”:
{steps}

ãƒ¬ãƒ“ãƒ¥ãƒ¼:
{review}

æœ€çµ‚çš„ã«çµ±åˆã—ãŸå›ç­”:
"""
    )
    steps_text = "\n".join([f"{i}. {s} â†’ {a}" for i, (s, a) in enumerate(step_results, start=1)])
    agg_chain = agg_prompt | llm
    return agg_chain.invoke({"question": question, "steps": steps_text, "review": review_notes})

# ======== Streamlit UI & ãƒ¡ã‚¤ãƒ³å‡¦ç† ========
def main():
    config = get_config()
    embeddings = load_embeddings(config)
    llm = load_llm(config)
    planner_chain = get_planner_chain(llm)

    st.title("ğŸ“š Steps-RAG")

    if st.button("ğŸ”„ ChromaDB å†æ§‹ç¯‰"):
        build_chroma(embeddings, config)
        st.success("ChromaDBã‚’å†æ§‹ç¯‰ã—ã¾ã—ãŸ")

    question = st.text_input("ğŸ’¬ è³ªå•ã‚’å…¥åŠ›:")
    if st.button("ğŸ§  å›ç­”ç”Ÿæˆ") and question:
        vectorstore = Chroma(persist_directory=config["CHROMA_PATH"], embedding_function=embeddings)
        # Step 1: Plan
        st.subheader("ğŸ“ Step 1: Plan")
        plan_obj = planner_chain.invoke({"question": question})
        st.write(plan_obj)
        steps = plan_obj.steps if hasattr(plan_obj, "steps") else []
        # Step 2: Retrieve & Answer
        st.subheader("ğŸ” Step 2: Retrieve & Answer")
        step_results = retrieve_and_answer(vectorstore, steps, llm)
        # for i, (s, a) in enumerate(step_results, start=1):
        #     st.markdown(f"**Step {i}: {s}**")
        #     st.write(a)
        # Step 3: Review
        st.subheader("ğŸ” Step 3: Review")
        review = review_steps(step_results, llm)
        st.write(review.content)
        # Step 4: Aggregate
        st.subheader("âœ… Step 4: Final Answer")
        final = aggregate_answer(question, step_results, review, llm)
        st.write(final.content)

if __name__ == "__main__":
    main()
