import streamlit as st
import os
import json
import uuid
from typing import List, Dict, Any, TypedDict, Annotated

# Langchain/Langgraph imports
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings # OpenAIã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.chat_models import ChatOllama # Ollamaã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from langchain_openai import OpenAIEmbeddings # Chromadbç”¨ã«OpenAI Embeddingsã¯æ®‹ã™
# from langchain_community.embeddings import OllamaEmbeddings # Ollama Embeddingsã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ
from langgraph.graph import StateGraph, END

# Chromadb imports
import chromadb
from chromadb.utils import embedding_functions

# --- Configuration ---
# LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®é¸æŠ
LLM_PROVIDER = st.sidebar.selectbox("LLM Provider", ["Ollama", "OpenAI"])

# Ollamaè¨­å®š
if LLM_PROVIDER == "Ollama":
    ollama_base_url = st.sidebar.text_input("Ollama Base URL", "http://localhost:11434")
    ollama_model_name = st.sidebar.text_input("Ollama Model Name", "gemma3:4b-it-qat") # ä½¿ç”¨ã™ã‚‹Ollamaãƒ¢ãƒ‡ãƒ«å
    if not ollama_base_url or not ollama_model_name:
        st.error("Ollama Base URL ã¨ Model Name ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        st.stop()
    # Ollama Chat Modelã®åˆæœŸåŒ–
    llm = ChatOllama(base_url=ollama_base_url, model=ollama_model_name, temperature=0)
    st.sidebar.success(f"Ollama ({ollama_model_name}) ã‚’LLMã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚")

# OpenAIè¨­å®š
elif LLM_PROVIDER == "OpenAI":
    # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰OpenAI APIã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã‚€
    # Streamlit Secretsã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ st.secrets["OPENAI_API_KEY"] ã«å¤‰æ›´
    openai_api_key = os.getenv("OPENAI_API_KEY")
    if not openai_api_key:
        st.error("OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ç’°å¢ƒå¤‰æ•° 'OPENAI_API_KEY' ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        st.stop()
    # OpenAI Chat Modelã®åˆæœŸåŒ–
    llm = ChatOpenAI(model="gpt-4o-mini", api_key=openai_api_key, temperature=0) # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®š
    st.sidebar.success("OpenAI ã‚’LLMã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚")

# Embeddingãƒ¢ãƒ‡ãƒ«ã®æŒ‡å®š (Chromadbç”¨)
# ç¾åœ¨ã¯OpenAI Embeddingsã‚’ä½¿ç”¨ã€‚Ollama Embeddingsã«å¤‰æ›´ã™ã‚‹å ´åˆã¯ä»¥ä¸‹ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ã€
# langchain_community.embeddings.OllamaEmbeddings ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦ä½¿ç”¨ã™ã‚‹ã€‚
# EMBEDDING_MODEL = "text-embedding-ada-002"
# embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL, api_key=openai_api_key) # OpenAI APIã‚­ãƒ¼ãŒå¿…è¦

# Ollama Embeddingsã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã®ä¾‹:
# EMBEDDING_MODEL = "nomic-embed-text" # ä½¿ç”¨ã™ã‚‹Ollama Embeddingãƒ¢ãƒ‡ãƒ«å
# embeddings = OllamaEmbeddings(base_url=ollama_base_url, model=EMBEDDING_MODEL)

# HuggingFace Embeddingsã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã®ä¾‹:
embeddings = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-small") # HuggingFaceã®åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®š

# Chromadbã®åˆæœŸåŒ–
# ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ç”¨ã€‚æ°¸ç¶šåŒ–ã™ã‚‹å ´åˆã¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®š
client = chromadb.Client() # ã¾ãŸã¯ chromadb.PersistentClient(path="./chroma_db")

# ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å
COLLECTION_NAME = "par_rag_documents"

# --- Data Preparation (Sample Data) ---
# RAGã«ä½¿ç”¨ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ–‡æ›¸ãƒ‡ãƒ¼ã‚¿
sample_documents = [
    "PAR RAGã¯ã€ãƒãƒ«ãƒãƒ›ãƒƒãƒ—è³ªå•å¿œç­”ã®æ¨è«–çµŒè·¯ã®ãšã‚Œã‚„ã‚¨ãƒ©ãƒ¼ä¼æ’­ã‚’è»½æ¸›ã™ã‚‹ãŸã‚ã«ææ¡ˆã•ã‚Œã¾ã—ãŸã€‚",
    "PAR RAGã¯Plan, Action, Reviewã®3ã¤ã®ä¸»è¦ãªæ®µéšã§æ§‹æˆã•ã‚Œã¾ã™ã€‚",
    "Plan Moduleã¯ã€è¤‡é›‘ãªå•é¡Œã‚’åˆ†è§£ã—ã€å®Ÿè¡Œå¯èƒ½ãªå¤šæ®µéšã®è¨ˆç”»ã‚’ãƒˆãƒƒãƒ—ãƒ€ã‚¦ãƒ³æ–¹å¼ã§ç”Ÿæˆã—ã¾ã™ã€‚",
    "Action Moduleã¯ã€è¨ˆç”»ã®å„ã‚¹ãƒ†ãƒƒãƒ—ã‚’é †åºé€šã‚Šã«å®Ÿè¡Œã—ã€æƒ…å ±æ¤œç´¢ã¨æš«å®šå›ç­”ç”Ÿæˆã‚’è¡Œã„ã¾ã™ã€‚",
    "Review Moduleã¯ã€å®Ÿè¡Œæ®µéšã§ç”Ÿæˆã•ã‚ŒãŸæš«å®šçš„ãªå›ç­”ã‚’æ¤œè¨¼ã—ã€å¿…è¦ã«å¿œã˜ã¦ä¿®æ­£ã—ã¾ã™ã€‚",
    "Review Moduleã¯ã€ç´°ç²’åº¦ã®æƒ…å ±æ¤œç´¢ã«ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã‚’åˆ©ç”¨ã—ã¾ã™ã€‚",
    "ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã¯ã€æ–‡æ›¸ã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã¨ãã®é–¢ä¿‚ã§æ§‹ç¯‰ã•ã‚Œã¾ã™ã€‚",
    "å¤šç²’åº¦æ¤œç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨ã—ã¦ã€ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼æ€§æ¤œç´¢ã¨Personalized PageRank (PPR) ãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚",
    "PAR RAGã®è¨­è¨ˆæ€æƒ³ã¯ã€äººé–“ã®PDCAã‚µã‚¤ã‚¯ãƒ«ã‹ã‚‰ç€æƒ³ã‚’å¾—ã¦ã„ã¾ã™ã€‚",
    "è¨ˆç”»ã¯ã€å„ã‚¹ãƒ†ãƒƒãƒ—ã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ (thought) ã¨ã‚µãƒ–è³ªå• (question) ã‚’å«ã‚€æ§‹é€ ã§å®šç¾©ã•ã‚Œã¾ã™ã€‚",
    "Action Moduleã§ã¯ã€æš«å®šå›ç­”ç”Ÿæˆæ™‚ã«å¼•ç”¨ã«åŸºã¥ãè¨¼æ‹ é¸æŠãŒè¡Œã‚ã‚Œã¾ã™ã€‚",
    "Action Moduleã§ã¯ã€ç¾åœ¨ã®æš«å®šå›ç­”ã‚’ç”¨ã„ã¦æ¬¡ã‚¹ãƒ†ãƒƒãƒ—ã®è³ªå•ã‚’æ´—ç·´ã—ã¾ã™ã€‚",
    "å„ã‚¹ãƒ†ãƒƒãƒ—ã®å®Ÿè¡Œå®Œäº†å¾Œã€è»Œè·¡ (trajectory) ãŒç”Ÿæˆã•ã‚Œã€è»Œè·¡ãƒã‚§ãƒ¼ãƒ³ã«è¿½åŠ ã•ã‚Œã¾ã™ã€‚",
    "å…¨ã¦ã®è¨ˆç”»ã‚¹ãƒ†ãƒƒãƒ—å®Œäº†å¾Œã€è»Œè·¡ãƒã‚§ãƒ¼ãƒ³ã‚’ç”¨ã„ã¦æœ€çµ‚çš„ãªå›ç­”ãŒç”Ÿæˆã•ã‚Œã¾ã™ã€‚",
    "PAR RAGã¯ã€ãƒãƒ«ãƒãƒ›ãƒƒãƒ—QAã‚¿ã‚¹ã‚¯ã§EMãŠã‚ˆã³F1ã‚¹ã‚³ã‚¢ã®å‘ä¸Šã‚’é”æˆã—ã¾ã—ãŸã€‚",
    "PAR RAGã®èª²é¡Œã¨ã—ã¦ã€å¿œç­”æ™‚é–“ï¼ˆRTPQï¼‰ã¨ã‚³ã‚¹ãƒˆï¼ˆCTPQï¼‰ã®å¢—åŠ ãŒæŒ™ã’ã‚‰ã‚Œã¾ã™ã€‚"
]

# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’Chromadbã«æŠ•å…¥ã™ã‚‹é–¢æ•°
def setup_chromadb(docs: List[str], collection_name: str, embedding_func):
    # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãŒå­˜åœ¨ã™ã‚Œã°å‰Šé™¤
    try:
        client.delete_collection(name=collection_name)
    except:
        pass # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½•ã‚‚ã—ãªã„

    # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
    # embedding_func ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§æ¸¡ã•ã‚ŒãŸEmbeddingFunctionã‚’ä½¿ç”¨
    collection = client.create_collection(name=collection_name, embedding_function=embedding_func)

    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨IDã‚’æº–å‚™
    ids = [str(uuid.uuid4()) for _ in docs]
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿å­˜ (å¼•ç”¨æ™‚ã«ä½¿ç”¨)
    metadatas = [{"text": doc} for doc in docs]

    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«è¿½åŠ 
    collection.add(
        documents=docs,
        metadatas=metadatas,
        ids=ids
    )
    st.sidebar.success(f"Chromadbã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{collection_name}' ã« {len(docs)} ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸã€‚")
    return collection

# Chromadbã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®å–å¾—ã¾ãŸã¯ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
# EmbeddingFunctionã‚’æ¸¡ã™å¿…è¦ãŒã‚ã‚‹ãŸã‚ã€HuggingFaceEmbeddingFunctionã‚’ã“ã“ã§å®šç¾©
# chroma_embedding_func = embedding_functions.OpenAIEmbeddingFunction(
#     api_key=openai_api_key, model_name="text-embedding-ada-002"
# )  # OpenAI Embeddingsã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã®ä¾‹ (ç„¡åŠ¹åŒ–)

chroma_embedding_func = embedding_functions.HuggingFaceEmbeddingFunction(
    model_name="intfloat/multilingual-e5-small"  # å…ˆã«å®šç¾©ã—ãŸHuggingFace Embeddingsã¨ä¸€è‡´ã•ã›ã‚‹
)


try:
    collection = client.get_collection(name=COLLECTION_NAME)
    st.sidebar.info(f"æ—¢å­˜ã®Chromadbã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{COLLECTION_NAME}' ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
except:
    st.sidebar.info(f"Chromadbã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{COLLECTION_NAME}' ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã—ã¾ã™ã€‚")
    # setup_chromadb é–¢æ•°ã« embedding_func ã‚’æ¸¡ã™
    collection = setup_chromadb(sample_documents, COLLECTION_NAME, chroma_embedding_func)


# --- Langgraph State ---
# ã‚°ãƒ©ãƒ•ã®çŠ¶æ…‹ã‚’å®šç¾©
class GraphState(TypedDict):
    """
    Represents the state of our graph.

    Attributes:
        question: The user's initial question.
        plan: The multi-step plan generated by the Plan Module.
        current_step_index: The index of the current step being executed in the plan.
        trajectory_chain: A list of dictionaries, where each dictionary contains
                          the question, provisional answer, and citations for a step.
        final_answer: The final answer generated after executing all steps.
        error: Any error message encountered during execution.
    """
    question: str
    plan: List[Dict[str, Any]]
    current_step_index: int
    trajectory_chain: List[Dict[str, Any]]
    final_answer: str
    error: str

# --- Langgraph Nodes (Modules) ---

# 1. Plan Module
def plan_module(state: GraphState) -> GraphState:
    """
    Generates a multi-step plan based on the user's question.
    """
    st.subheader("ğŸš€ ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°æ®µéš")
    question = state["question"]
    st.write(f"è³ªå•: {question}")

    plan_prompt = PromptTemplate(
        template="""ã‚ãªãŸã¯è¤‡é›‘ãªè³ªå•ã‚’è§£æ±ºã™ã‚‹ãŸã‚ã®å°‚é–€å®¶ã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€ãã‚Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã®å¤šæ®µéšã®è¨ˆç”»ã‚’ãƒˆãƒƒãƒ—ãƒ€ã‚¦ãƒ³æ–¹å¼ã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
è¨ˆç”»ã¯JSONå½¢å¼ã§å‡ºåŠ›ã—ã€å„ã‚¹ãƒ†ãƒƒãƒ—ã«ã¯ 'thought' (ãã®ã‚¹ãƒ†ãƒƒãƒ—ã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹) ã¨ 'question' (ãã®ã‚¹ãƒ†ãƒƒãƒ—ã§è§£æ±ºã™ã¹ãã‚µãƒ–è³ªå•) ã‚’å«ã‚ã¦ãã ã•ã„ã€‚
è¨ˆç”»ã¯å®Ÿè¡Œå¯èƒ½ã§ã€æœ€çµ‚çš„ã«å…ƒã®è³ªå•ã«å›ç­”ã§ãã‚‹ã‚ˆã†å°ãã‚‚ã®ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
è³ªå•ã®è¤‡é›‘ã•ã«å¿œã˜ã¦ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚

è³ªå•: {question}

è¨ˆç”»ã®JSONå½¢å¼ä¾‹:
[
  {{
    "thought": "ã¾ãšã€æœ€åˆã®ã‚µãƒ–å•é¡Œã‚’ç‰¹å®šã—ã¾ã™ã€‚",
    "question": "æœ€åˆã®ã‚µãƒ–è³ªå•"
  }},
  {{
    "thought": "æ¬¡ã«ã€å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã®çµæœã‚’ä½¿ã£ã¦æ¬¡ã®ã‚µãƒ–å•é¡Œã‚’è§£æ±ºã—ã¾ã™ã€‚",
    "question": "æ¬¡ã®ã‚µãƒ–è³ªå•"
  }},
  ...
]
""",
        input_variables=["question"],
    )

    # LLMã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ä½¿ç”¨
    chain = plan_prompt | llm | JsonOutputParser()

    try:
        plan = chain.invoke({"question": question})
        st.write("ç”Ÿæˆã•ã‚ŒãŸè¨ˆç”»:")
        st.json(plan)
        return {**state, "plan": plan, "current_step_index": 0, "trajectory_chain": [], "final_answer": "", "error": ""}
    except Exception as e:
        st.error(f"ãƒ—ãƒ©ãƒ³ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return {**state, "error": f"ãƒ—ãƒ©ãƒ³ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"}


# 2. Action Module
def action_module(state: GraphState) -> GraphState:
    """
    Executes a single step of the plan: retrieves information, generates a provisional answer,
    selects citations, and refines the next question.
    """
    st.subheader(f"ğŸ”¬ å®Ÿè¡Œæ®µéš (ã‚¹ãƒ†ãƒƒãƒ— {state['current_step_index'] + 1})")
    question = state["question"]
    plan = state["plan"]
    current_step_index = state["current_step_index"]
    trajectory_chain = state["trajectory_chain"]

    if current_step_index >= len(plan):
        st.warning("è¨ˆç”»ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°ãŒè¶…éã—ã¾ã—ãŸã€‚")
        return {**state, "error": "è¨ˆç”»ã‚¹ãƒ†ãƒƒãƒ—è¶…é"}

    current_step = plan[current_step_index]
    sub_question = current_step["question"]
    st.write(f"ç¾åœ¨ã®ã‚µãƒ–è³ªå•: {sub_question}")

    # ç²—ç²’åº¦æ¤œç´¢ (Chromadbã‚’ä½¿ç”¨)
    st.info("æƒ…å ±ã‚’æ¤œç´¢ä¸­...")
    try:
        # æ¤œç´¢ã‚¯ã‚¨ãƒªã¨ã—ã¦ã‚µãƒ–è³ªå•ã‚’ä½¿ç”¨
        # collection ã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ä½¿ç”¨
        results = collection.query(
            query_texts=[sub_question],
            n_results=5, # å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            include=['metadatas'] # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆå…ƒã®ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã‚’å«ã‚ã‚‹
        )
        retrieved_docs = [res['text'] for res in results['metadatas'][0]]
        st.write("æ¤œç´¢çµæœ:")
        for i, doc in enumerate(retrieved_docs):
            st.write(f"- {doc[:100]}...") # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å†’é ­ã‚’è¡¨ç¤º
    except Exception as e:
        st.error(f"æƒ…å ±æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return {**state, "error": f"æƒ…å ±æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}"}

    # æš«å®šå›ç­”ç”Ÿæˆã¨å¼•ç”¨é¸æŠ
    st.info("æš«å®šå›ç­”ã‚’ç”Ÿæˆä¸­...")
    answer_prompt = PromptTemplate(
        template="""ä»¥ä¸‹ã®æƒ…å ±ã¨è³ªå•ã«åŸºã¥ã„ã¦ã€æš«å®šçš„ãªå›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
å›ç­”ã«ã¯ã€æä¾›ã•ã‚ŒãŸæƒ…å ±ã®ä¸­ã‹ã‚‰å›ç­”ã®æ ¹æ‹ ã¨ãªã‚‹éƒ¨åˆ†ã‚’å¼•ç”¨ã¨ã—ã¦å«ã‚ã¦ãã ã•ã„ã€‚
å¼•ç”¨ã¯ã€å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ãã®ã¾ã¾ä½¿ç”¨ã—ã€å¿…è¦ã«å¿œã˜ã¦æ–‡è„ˆã«åˆã‚ã›ã¦èª¿æ•´ã—ã¦ãã ã•ã„ã€‚
ã‚‚ã—æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯ã€ãã®æ—¨ã‚’å›ç­”ã«å«ã‚ã¦ãã ã•ã„ã€‚

æƒ…å ±:
{context}

è³ªå•: {question}

æš«å®šå›ç­”ã¨å¼•ç”¨:
""",
        input_variables=["context", "question"],
    )

    context = "\n".join(retrieved_docs)
    # LLMã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ä½¿ç”¨
    chain = answer_prompt | llm

    try:
        provisional_answer_with_citations = chain.invoke({"context": context, "question": sub_question}).content
        st.write("æš«å®šå›ç­”ã¨å¼•ç”¨:")
        st.write(provisional_answer_with_citations)

        # æš«å®šå›ç­”ã‹ã‚‰å¼•ç”¨éƒ¨åˆ†ã¨å›ç­”éƒ¨åˆ†ã‚’åˆ†é›¢ã™ã‚‹ï¼ˆç°¡æ˜“çš„ãªæ–¹æ³•ï¼‰
        # ã‚ˆã‚Šå³å¯†ãªå¼•ç”¨æŠ½å‡ºã¯LLMã«JSONå½¢å¼ã§å‡ºåŠ›ã•ã›ã‚‹ãªã©å·¥å¤«ãŒå¿…è¦
        provisional_answer = provisional_answer_with_citations # ä¸€æ—¦å…¨ä½“ã‚’æš«å®šå›ç­”ã¨ã™ã‚‹
        citations = retrieved_docs # æ¤œç´¢ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå…¨ä½“ã‚’å¼•ç”¨ã¨ã—ã¦æ‰±ã†ï¼ˆç°¡æ˜“åŒ–ï¼‰

    except Exception as e:
        st.error(f"æš«å®šå›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return {**state, "error": f"æš«å®šå›ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"}

    # æ¬¡ã‚¹ãƒ†ãƒƒãƒ—ã®è³ªå•æ´—ç·´ (ã‚‚ã—æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ãŒã‚ã‚Œã°)
    refined_next_question = None
    if current_step_index + 1 < len(plan):
        st.info("æ¬¡ã‚¹ãƒ†ãƒƒãƒ—ã®è³ªå•ã‚’æ´—ç·´ä¸­...")
        next_step = plan[current_step_index + 1]
        original_next_question = next_step["question"]

        refine_prompt = PromptTemplate(
            template="""ã‚ãªãŸã¯è³ªå•æ´—ç·´ã®å°‚é–€å®¶ã§ã™ã€‚
ç¾åœ¨ã®æš«å®šçš„ãªå›ç­”ã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã®å…ƒã®è³ªå•ã«åŸºã¥ã„ã¦ã€æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã®è³ªå•ã‚’æ´—ç·´ã—ã¦ãã ã•ã„ã€‚
æ´—ç·´ã•ã‚ŒãŸè³ªå•ã¯ã€ç¾åœ¨ã®å›ç­”ã®æ–‡è„ˆã‚’è€ƒæ…®ã—ã€ã‚ˆã‚Šå…·ä½“çš„ã§åŠ¹æœçš„ãªæƒ…å ±æ¤œç´¢ã‚„æ¨è«–ã‚’å¯èƒ½ã«ã™ã‚‹ã‚ˆã†ã«èª¿æ•´ã—ã¦ãã ã•ã„ã€‚
æ´—ç·´ã•ã‚ŒãŸè³ªå•ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

ç¾åœ¨ã®æš«å®šå›ç­”: {provisional_answer}

æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã®å…ƒã®è³ªå•: {original_next_question}

æ´—ç·´ã•ã‚ŒãŸæ¬¡ã®è³ªå•:
""",
            input_variables=["provisional_answer", "original_next_question"],
        )
        # LLMã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ä½¿ç”¨
        chain = refine_prompt | llm

        try:
            refined_next_question = chain.invoke({"provisional_answer": provisional_answer, "original_next_question": original_next_question}).content
            st.write(f"æ´—ç·´ã•ã‚ŒãŸæ¬¡ã®è³ªå•: {refined_next_question}")
            # è¨ˆç”»ã®æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã®è³ªå•ã‚’æ›´æ–°
            plan[current_step_index + 1]["question"] = refined_next_question
        except Exception as e:
            st.warning(f"æ¬¡ã‚¹ãƒ†ãƒƒãƒ—ã®è³ªå•æ´—ç·´ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚å‡¦ç†ã¯ç¶šè¡Œ
            refined_next_question = original_next_question # æ´—ç·´ã«å¤±æ•—ã—ãŸã‚‰å…ƒã®è³ªå•ã‚’ä½¿ç”¨

    # è»Œè·¡ã®ç”Ÿæˆã¨è¿½åŠ 
    trajectory = {
        "step": current_step_index + 1,
        "sub_question": sub_question,
        "provisional_answer": provisional_answer,
        "citations": citations
    }
    trajectory_chain.append(trajectory)
    st.write("è»Œè·¡ã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚")

    # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¸é€²ã‚€
    next_step_index = current_step_index + 1

    return {
        **state,
        "plan": plan, # æ´—ç·´ã•ã‚ŒãŸè³ªå•ã§æ›´æ–°ã•ã‚ŒãŸè¨ˆç”»
        "current_step_index": next_step_index,
        "trajectory_chain": trajectory_chain,
        "error": ""
    }

# 3. Review Module
def review_module(state: GraphState) -> GraphState:
    """
    Reviews the provisional answer generated in the Action Module,
    validates its accuracy, and potentially revises it.
    """
    st.subheader(f"ğŸ§ ãƒ¬ãƒ“ãƒ¥ãƒ¼æ®µéš (ã‚¹ãƒ†ãƒƒãƒ— {state['current_step_index']})") # Actionã®å¾Œã«å‘¼ã°ã‚Œã‚‹ãŸã‚ã€indexã¯Actionã§ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ãƒˆã•ã‚ŒãŸå¾Œ
    question = state["question"]
    plan = state["plan"]
    current_step_index = state["current_step_index"] - 1 # ãƒ¬ãƒ“ãƒ¥ãƒ¼å¯¾è±¡ã¯ç›´å‰ã®ã‚¹ãƒ†ãƒƒãƒ—
    trajectory_chain = state["trajectory_chain"]

    if not trajectory_chain:
        st.warning("ãƒ¬ãƒ“ãƒ¥ãƒ¼å¯¾è±¡ã®è»Œè·¡ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return state # ãƒ¬ãƒ“ãƒ¥ãƒ¼ã™ã‚‹ã‚‚ã®ãŒãªã„å ´åˆã¯ãã®ã¾ã¾è¿”ã™

    last_trajectory = trajectory_chain[-1]
    sub_question = last_trajectory["sub_question"]
    provisional_answer = last_trajectory["provisional_answer"]
    st.write(f"ãƒ¬ãƒ“ãƒ¥ãƒ¼å¯¾è±¡ã®ã‚µãƒ–è³ªå•: {sub_question}")
    st.write(f"ãƒ¬ãƒ“ãƒ¥ãƒ¼å¯¾è±¡ã®æš«å®šå›ç­”: {provisional_answer}")

    # ç´°ç²’åº¦æ¤œç´¢ (ã“ã“ã§ã¯æš«å®šå›ç­”ã‚’ã‚¯ã‚¨ãƒªã¨ã—ã¦Chromadbã‚’ä½¿ç”¨)
    # æœ¬æ¥ã¯KGã‚‚åˆ©ç”¨ã™ã‚‹ãŒã€ç°¡æ˜“åŒ–ã®ãŸã‚Chromadbã®ã¿
    st.info("ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ãŸã‚ã®æƒ…å ±ã‚’æ¤œç´¢ä¸­...")
    try:
        # æ¤œç´¢ã‚¯ã‚¨ãƒªã¨ã—ã¦æš«å®šå›ç­”ã‚’ä½¿ç”¨
        # collection ã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ä½¿ç”¨
        results = collection.query(
            query_texts=[provisional_answer],
            n_results=3, # ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ãŸã‚ã®å°‘æ•°ã®é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
            include=['metadatas']
        )
        review_docs = [res['text'] for res in results['metadatas'][0]]
        st.write("ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨æ¤œç´¢çµæœ:")
        for i, doc in enumerate(review_docs):
            st.write(f"- {doc[:100]}...")
    except Exception as e:
        st.warning(f"ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨æƒ…å ±æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        review_docs = [] # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¯ç¶šè¡Œ

    # æš«å®šå›ç­”ã®æ¤œè¨¼ã¨ä¿®æ­£
    st.info("æš«å®šå›ç­”ã‚’æ¤œè¨¼ãƒ»ä¿®æ­£ä¸­...")
    review_prompt = PromptTemplate(
        template="""ã‚ãªãŸã¯å›ç­”æ¤œè¨¼ã®å°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®å…ƒã®è³ªå•ã€æš«å®šçš„ãªå›ç­”ã€ãã—ã¦ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ãŸã‚ã®é–¢é€£æƒ…å ±ã‚’è€ƒæ…®ã—ã¦ã€æš«å®šå›ç­”ã®æ­£ç¢ºæ€§ã‚’æ¤œè¨¼ã—ã¦ãã ã•ã„ã€‚
ã‚‚ã—æš«å®šå›ç­”ãŒä¸æ­£ç¢ºã€ä¸å®Œå…¨ã€ã¾ãŸã¯æä¾›ã•ã‚ŒãŸæƒ…å ±ã¨çŸ›ç›¾ã—ã¦ã„ã‚‹å ´åˆã€é–¢é€£æƒ…å ±ã«åŸºã¥ã„ã¦å›ç­”ã‚’ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚
æ¤œè¨¼ã®çµæœã€å›ç­”ãŒæ­£ç¢ºã§ã‚ã‚Œã°ã€å…ƒã®æš«å®šå›ç­”ã‚’ãã®ã¾ã¾å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
ä¿®æ­£ãŒå¿…è¦ãªå ´åˆã¯ã€ä¿®æ­£å¾Œã®å›ç­”ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

å…ƒã®è³ªå•: {original_question}

ãƒ¬ãƒ“ãƒ¥ãƒ¼å¯¾è±¡ã®ã‚µãƒ–è³ªå•: {sub_question}

ãƒ¬ãƒ“ãƒ¥ãƒ¼å¯¾è±¡ã®æš«å®šå›ç­”: {provisional_answer}

ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ãŸã‚ã®é–¢é€£æƒ…å ±:
{review_context}

æ¤œè¨¼ã¨ä¿®æ­£å¾Œã®å›ç­”:
""",
        input_variables=["original_question", "sub_question", "provisional_answer", "review_context"],
    )

    review_context = "\n".join(review_docs)
    # LLMã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ä½¿ç”¨
    chain = review_prompt | llm

    try:
        revised_answer = chain.invoke({
            "original_question": question,
            "sub_question": sub_question,
            "provisional_answer": provisional_answer,
            "review_context": review_context
        }).content
        st.write("æ¤œè¨¼ãƒ»ä¿®æ­£å¾Œã®å›ç­”:")
        st.write(revised_answer)

        # è»Œè·¡ãƒã‚§ãƒ¼ãƒ³ã®æœ€å¾Œã®å›ç­”ã‚’æ›´æ–°
        trajectory_chain[-1]["provisional_answer"] = revised_answer

    except Exception as e:
        st.warning(f"æš«å®šå›ç­”ã®æ¤œè¨¼ãƒ»ä¿®æ­£ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚è»Œè·¡ã¯æ›´æ–°ã—ãªã„

    return {**state, "trajectory_chain": trajectory_chain, "error": ""}


# 4. Final Answer Module
def final_answer_module(state: GraphState) -> GraphState:
    """
    Generates the final answer based on the initial question and the trajectory chain.
    """
    st.subheader("âœ¨ æœ€çµ‚å›ç­”ç”Ÿæˆæ®µéš")
    question = state["question"]
    trajectory_chain = state["trajectory_chain"]

    st.write("è»Œè·¡ãƒã‚§ãƒ¼ãƒ³å…¨ä½“ã‚’è€ƒæ…®ã—ã¦æœ€çµ‚å›ç­”ã‚’ç”Ÿæˆä¸­...")

    # è»Œè·¡ãƒã‚§ãƒ¼ãƒ³ã‚’æ•´å½¢ã—ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å«ã‚ã‚‹
    trajectory_text = ""
    for traj in trajectory_chain:
        trajectory_text += f"--- ã‚¹ãƒ†ãƒƒãƒ— {traj['step']} ---\n"
        trajectory_text += f"ã‚µãƒ–è³ªå•: {traj['sub_question']}\n"
        trajectory_text += f"æš«å®šå›ç­”: {traj['provisional_answer']}\n"
        # å¼•ç”¨ã¯å†—é•·ã«ãªã‚‹ãŸã‚ã“ã“ã§ã¯å«ã‚ãªã„ãŒã€å¿…è¦ã«å¿œã˜ã¦è¿½åŠ 
        # trajectory_text += f"å¼•ç”¨: {', '.join(traj['citations'])}\n"
        trajectory_text += "\n"

    final_answer_prompt = PromptTemplate(
        template="""ã‚ãªãŸã¯æœ€çµ‚å›ç­”ç”Ÿæˆã®å°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®å…ƒã®è³ªå•ã¨ã€ãã‚Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«å®Ÿè¡Œã•ã‚ŒãŸå¤šæ®µéšã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ï¼ˆè»Œè·¡ãƒã‚§ãƒ¼ãƒ³ï¼‰ã‚’è€ƒæ…®ã—ã¦ã€æœ€çµ‚çš„ãªå›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
è»Œè·¡ãƒã‚§ãƒ¼ãƒ³ã«ã¯å„ã‚¹ãƒ†ãƒƒãƒ—ã§ã®ã‚µãƒ–è³ªå•ã¨æš«å®šå›ç­”ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚
ã“ã‚Œã‚‰ã®æƒ…å ±ã‚’ã‚‚ã¨ã«ã€å…ƒã®è³ªå•ã«å¯¾ã™ã‚‹åŒ…æ‹¬çš„ã§æ­£ç¢ºãªæœ€çµ‚å›ç­”ã‚’ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚

å…ƒã®è³ªå•: {question}

è»Œè·¡ãƒã‚§ãƒ¼ãƒ³:
{trajectory_chain}

æœ€çµ‚å›ç­”:
""",
        input_variables=["question", "trajectory_chain"],
    )

    # LLMã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ä½¿ç”¨
    chain = final_answer_prompt | llm

    try:
        final_answer = chain.invoke({"question": question, "trajectory_chain": trajectory_text}).content
        st.write("æœ€çµ‚å›ç­”:")
        st.success(final_answer)
        return {**state, "final_answer": final_answer, "error": ""}
    except Exception as e:
        st.error(f"æœ€çµ‚å›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return {**state, "error": f"æœ€çµ‚å›ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"}

# --- Langgraph Graph Definition ---

# ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰
workflow = StateGraph(GraphState)

# ãƒãƒ¼ãƒ‰ã‚’è¿½åŠ 
workflow.add_node("plan", plan_module)
workflow.add_node("action", action_module)
workflow.add_node("review", review_module)
workflow.add_node("final_answer", final_answer_module)

# ã‚¨ãƒƒã‚¸ï¼ˆé·ç§»ï¼‰ã‚’å®šç¾©

# é–‹å§‹ãƒãƒ¼ãƒ‰
workflow.set_entry_point("plan")

# plan -> action
workflow.add_edge("plan", "action")

# action -> review
workflow.add_edge("action", "review")

# review ã‹ã‚‰ã®æ¡ä»¶ä»˜ãé·ç§»
# æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ãŒã‚ã‚‹ã‹ã€ã¾ãŸã¯ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸã‹ã§é·ç§»å…ˆã‚’æ±ºå®š
def should_continue(state: GraphState) -> str:
    """
    Determines whether to continue to the next action step or finish.
    """
    if state.get("error"):
        return "end" # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸã‚‰çµ‚äº†
    plan = state["plan"]
    current_step_index = state["current_step_index"]
    # current_step_index ã¯ action_module ã®æœ€å¾Œã§ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ãƒˆã•ã‚Œã¦ã„ã‚‹
    if current_step_index < len(plan):
        return "continue" # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ãŒã‚ã‚Œã° action ã¸
    else:
        return "end" # ãªã‘ã‚Œã° final_answer ã¸

workflow.add_conditional_edges(
    "review",
    should_continue,
    {
        "continue": "action",
        "end": "final_answer"
    }
)

# final_answer -> END
workflow.add_edge("final_answer", END)

# ã‚°ãƒ©ãƒ•ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
app = workflow.compile()

# --- Streamlit App UI ---

st.title("PAR RAG ãƒ‡ãƒ¢")
st.markdown("""
PAR RAG (Plan-Action-Review Retrieval-Augmented Generation) ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¢ãƒ—ãƒªã§ã™ã€‚
è¤‡é›‘ãªè³ªå•ã«å¯¾ã—ã¦ã€Plan, Action, Reviewã®æ®µéšã‚’çµŒã¦å›ç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’é¸æŠã§ãã¾ã™ã€‚
""")

# è³ªå•å…¥åŠ›
user_question = st.text_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:", "PAR RAGã®è¨­è¨ˆæ€æƒ³ã¨ä¸»è¦ãªæ©Ÿèƒ½è¦ä»¶ã¯ä½•ã§ã™ã‹ï¼Ÿ")

# å®Ÿè¡Œãƒœã‚¿ãƒ³
if st.button("å®Ÿè¡Œ"):
    if not user_question:
        st.warning("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        st.info("å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
        # ã‚°ãƒ©ãƒ•ã®å®Ÿè¡Œ
        # çŠ¶æ…‹ã®åˆæœŸåŒ–
        initial_state = {
            "question": user_question,
            "plan": [],
            "current_step_index": 0,
            "trajectory_chain": [],
            "final_answer": "",
            "error": ""
        }

        # ã‚°ãƒ©ãƒ•ã®å®Ÿè¡Œã¨çµæœã®è¡¨ç¤º
        # ã‚¹ãƒˆãƒªãƒ¼ãƒ è¡¨ç¤ºã«ã¯å¯¾å¿œã—ã¦ã„ãªã„ç°¡æ˜“çš„ãªå®Ÿè¡Œ
        try:
            # å®Ÿè¡Œä¸­ã«å„ãƒãƒ¼ãƒ‰ã‹ã‚‰ã®å‡ºåŠ›ãŒStreamlitã«è¡¨ç¤ºã•ã‚Œã‚‹
            # LLMã¨collectionã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ã‚¢ã‚¯ã‚»ã‚¹ã•ã‚Œã‚‹
            final_state = app.invoke(initial_state)

            # æœ€çµ‚çµæœã®ç¢ºèª (final_answer_moduleã§è¡¨ç¤ºæ¸ˆã¿ã ãŒã€å¿µã®ãŸã‚)
            # if final_state.get("final_answer"):
            #     st.subheader("æœ€çµ‚å›ç­”:")
            #     st.success(final_state["final_answer"])
            if final_state.get("error"):
                 st.error(f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {final_state['error']}")

            st.balloons() # å®Œäº†æ™‚ã«ãƒãƒ«ãƒ¼ãƒ³ã‚’è¡¨ç¤º

        except Exception as e:
            st.error(f"ã‚°ãƒ©ãƒ•å®Ÿè¡Œä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

st.markdown("---")
st.write("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¨Chromadbã®ãƒ­ãƒ¼ãƒ‰çŠ¶æ³ã‚’ç¢ºèªã§ãã¾ã™ã€‚")
